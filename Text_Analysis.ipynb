{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18791b09-5c07-48f9-9031-a20448e60c5c",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Text Analysis - Assignment</h1><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295ac7f-eb5c-427a-91be-77699bf9f809",
   "metadata": {},
   "source": [
    "## Dependencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d8df909-f751-4cfa-b8d4-3959e3144e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import openpyxl\n",
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "from nltk import word_tokenize , sent_tokenize \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18feb9b9-a66f-4360-bfdc-c96791bd3bd5",
   "metadata": {},
   "source": [
    "## Extracting Text AND Saving into file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72dcd641-9312-4e2e-812e-cfe1922b73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=\"Input (1).xlsx\"\n",
    "df=pd.read_excel(input_dir) #reading excel file\n",
    "URL_ID=df['URL_ID'].to_list()       #getting particluar columns' content in a list \n",
    "URL_LIST=df['URL'].to_list()\n",
    "\n",
    "def extract_data(URL): #to scrape data from Webpages\n",
    "    article=''\n",
    "    try:\n",
    "        response=requests.get(URL) # making get request\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')  #creating a soup object with response data\n",
    "        title=soup.find('h1').get_text() #getting title of webpage only\n",
    "        \n",
    "        content=soup.find_all(attrs={\"class\":\"td-post-content\"}) #getting only the required post content\n",
    "        for i in content:\n",
    "           article+= i.get_text()\n",
    "        text = title + '\\n' + article #merging title and text\n",
    "        return text\n",
    "    except:\n",
    "         print(f'Failed to get response from URL: {URL}' )\n",
    "        \n",
    "def save_text(URL , ID): # to save scraped data into files\n",
    "        try:\n",
    "            text = extract_data(URL)\n",
    "            file_name= ID +'.txt'\n",
    "            with open(file_name , 'w' ,encoding=\"utf-8\") as f: #file if not present will be created with the file name as URL ID\n",
    "                f.write(text) \n",
    "        except:\n",
    "            print('Unable to write to file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b6e14-ed39-4552-a98d-634eb85f1d10",
   "metadata": {},
   "source": [
    "## Data Analysis :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ca64f-db28-4134-9522-c8cb513cefea",
   "metadata": {},
   "source": [
    "### 1. Sentimental Analysis :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2cdcc7-7494-498d-8568-555d2aef8e3d",
   "metadata": {},
   "source": [
    "#### 1.1 Cleaning using Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b974fc58-ab68-44cb-8454-3f011c6e903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stword_dir='StopWords-20240505T100503Z-001/StopWords' #To open the directory to acccess files with stop words\n",
    "\n",
    "stop_words=[]                                             #All stop words will be included in this list\n",
    "for file in os.listdir(stword_dir):                       #iterating over list of files in particular directory\n",
    "    file_name = os.path.join(stword_dir, file)            #concatinating file name with the directory name\n",
    "    with open(file_name , 'r')as f: \n",
    "        words_list=f.read().replace('|','\\n').split(\"\\n\") #splitting string data into list of words in particular file\n",
    "        stop_words+=words_list                            #merging all lists of stopwords into a single list.\n",
    "\n",
    "def clean_text(tokenised_list):\n",
    "    cleaned_words=[word for word in tokenised_list if word.casefold() not in stop_words]   #filtering out all lowercase stopwords\n",
    "    cleaned_words=[word for word in cleaned_words if word.upper() not in stop_words]       #filtering out all uppercase stop words\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff62760-78ab-47fc-95b6-68ec0ab65180",
   "metadata": {},
   "source": [
    "##### WORD COUNT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "211b3187-7e72-41a5-9e96-ea1cf77180bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "                       #     ---Calculated Inside main() function---\n",
    "\n",
    "\n",
    "# word_tokens=word_tokenize(no_punct)   #Words list without punctuations\n",
    "# cleaned_words=clean_text(word_tokens) #Words list without punctuations and stop words\n",
    "# sent_tokens=sent_tokenize(textfile)   #List of sentences in the text\n",
    "\n",
    "\n",
    "# Num_clean_words=len(cleaned_words)\n",
    "# Num_sent=len(sent_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec7718-d179-422f-aafa-7c88dbd2cd54",
   "metadata": {},
   "source": [
    "#### 1.2 Postive and Negative words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020e7c11-940c-4148-9df1-3296b42c013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ngword_dir='MasterDictionary-20240505T100418Z-001/MasterDictionary/negative-words.txt'\n",
    "Psword_dir='MasterDictionary-20240505T100418Z-001/MasterDictionary/positive-words.txt'\n",
    "\n",
    "with open(Psword_dir , 'r') as f:\n",
    "    Ps_words=f.read().split('\\n')    #Creating positive words list\n",
    "    Pstv_words=clean_text(Ps_words)  #Filtering out stopwords\n",
    "    \n",
    "with open(Ngword_dir , 'r') as f:    #Creating negative words list\n",
    "    Ng_words=f.read().split('\\n')    #filtering out stopwords\n",
    "    Ngtv_words=clean_text(Ng_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96150428-cac3-441a-b81f-4030d151d352",
   "metadata": {},
   "source": [
    "#### 1.3 Calculating Derived Variables :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45af9c-72a7-4f24-a284-a9e581bb35a2",
   "metadata": {},
   "source": [
    "##### POSITIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe85813-a212-402f-b6f2-802184a77c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_score(filtered_list):\n",
    "    Pstv_score=0                          #initialising score\n",
    "    for word in filtered_list:\n",
    "        if word.casefold() in Pstv_words: \n",
    "            Pstv_score+=1                 #increment score +1 if word found\n",
    "    return Pstv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c326309-9d61-4c6c-8649-d07147e68630",
   "metadata": {},
   "source": [
    "##### NEGATIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd47e59a-5bcf-4ccc-b6fb-4bdce555a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_score(filtered_list):\n",
    "    Ngtv_score=0\n",
    "    for word in filtered_list:\n",
    "        if word.casefold() in Ngtv_words:\n",
    "            Ngtv_score-=1                   #increment score by -1 if word found\n",
    "    return -(Ngtv_score)                    #returning positive value of score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc02ddb-245c-4015-8d11-299109e7a7e0",
   "metadata": {},
   "source": [
    "##### POLARITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816adb19-ea8d-4dbc-8a77-8cecc38ded53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_score(ps_score, ng_score):\n",
    "    try:\n",
    "        Polarity_Score = (ps_score - ng_score)/ ((ps_score+ ng_score) + 0.000001)\n",
    "        return Polarity_Score\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d53c1-8c47-4d28-8cac-e26920b7da47",
   "metadata": {},
   "source": [
    "##### SUBJECTIVITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd527779-6196-4e71-8e32-ea3173bc902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity_score(ps_score, ng_score,cleaned_words):\n",
    "    try:\n",
    "        Subjectivity_Score = (ps_score + ng_score)/ ((len(cleaned_words)) + 0.000001)\n",
    "        return Subjectivity_Score\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38964e-221b-4ba1-928d-a3d920a81962",
   "metadata": {},
   "source": [
    "##### SYLLABLE COUNT PER WORD AND COMPLEX WORDS COUNT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dbec65c-0069-413b-956b-0b568b9aaa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word): #to count number of syllables in a word.\n",
    "    vowel='aeiou'\n",
    "    not_count=['ed','es']\n",
    "    count=0\n",
    "    word=word.casefold()\n",
    "    if word[-2:] not in not_count:  \n",
    "        for index in range(0,len(word)):\n",
    "            if word[index] in vowel:\n",
    "                count+=1\n",
    "    else:\n",
    "        for index in range(0,len(word)): \n",
    "            if word[index] in vowel:\n",
    "                count+=1\n",
    "        count=count-1      #not counting 'ed' and 'es' as a syllable.\n",
    "    return count\n",
    "\n",
    "def total_syllable_count(words_list):\n",
    "    total_count=0\n",
    "    for word in words_list:\n",
    "        count=syllable_count(word)\n",
    "        total_count+=count\n",
    "    return total_count\n",
    "\n",
    "def complex_words_count(words_list):\n",
    "    complex_words=[word for word in words_list if syllable_count(word) > 2]\n",
    "    cmplx_count=len(complex_words)\n",
    "    return cmplx_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019549b-00d0-4c0a-8ec5-86d3124dc854",
   "metadata": {},
   "source": [
    "##### AVERAGE SENTENCE LENGTH :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05820e2-4cbe-4cbd-96fe-40d1c9eaa4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sent_len(Num_clean_words,Num_sent):\n",
    "    try:\n",
    "        avg=Num_clean_words/Num_sent\n",
    "        return avg\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871282a9-1b3f-470c-9b91-ee8947eba571",
   "metadata": {},
   "source": [
    "##### PERCENTAGE COMPLEX WORDS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14fa4dbf-9cda-4362-bf85-3ba30142efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_cmplx(cmplx_count,Num_clean_words):\n",
    "    try:\n",
    "        per=(cmplx_count/Num_clean_words)*100\n",
    "        return per\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27b489-dcf3-4f70-be38-44fd5a0b4345",
   "metadata": {},
   "source": [
    "##### FOG INDEX :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e2ed33-4cdc-4b17-b940-28ef690a0a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog_index(per_cmplx,avg_sent_len):\n",
    "    fog_index=0.4*(avg_sent_len + per_cmplx)\n",
    "    return fog_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52de8f-4aa4-48dd-93bb-184fa2614773",
   "metadata": {},
   "source": [
    "##### PERSONAL PRONOUNS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd81fb97-76df-4b22-84cb-fe2006e588e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pers_pronoun(words_list):\n",
    "    pronouns=['I' , 'we' ,'We','My', 'my' , 'Ours','ours' , 'us' , 'Us' ]\n",
    "    count=0\n",
    "    for word in words_list:\n",
    "        if word in pronouns:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9d2c2-5c04-4dcc-8f18-17216f6a36d9",
   "metadata": {},
   "source": [
    "##### AVERGAE WORD LENGTH :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cef14e13-2a36-496a-8ba9-091c3093d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_chars(words_list):\n",
    "    total_chars=0\n",
    "    for word in words_list:\n",
    "        total_chars+=len(word)\n",
    "    return total_chars\n",
    "def avg_word_len(cleaned_words ,Num_clean_words):\n",
    "    try:\n",
    "        Num_chars=total_chars(cleaned_words)\n",
    "        Avg_word_len = Num_chars/Num_clean_words\n",
    "        return Avg_word_len\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1604493-63d6-453c-9052-41193d504d8a",
   "metadata": {},
   "source": [
    "## MAIN FUNCTION :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "939d1d97-97cd-4666-886f-be177af5b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    Output_data=[]\n",
    "    for URL , ID in zip(URL_LIST,URL_ID): #using zip to iterate over two lists simultaneously\n",
    "        save_text(URL , ID)    #extract from url and save text to file\n",
    "        file_name= ID + '.txt'\n",
    "        \n",
    "        with open(file_name,'r',encoding=\"utf8\") as f:\n",
    "            textfile=f.read()\n",
    "        no_punct = re.sub(r'[^\\w\\s]', '', textfile) #removing all punctuations  from text\n",
    "\n",
    "        word_tokens = word_tokenize(no_punct)   #Words list without punctuations\n",
    "        cleaned_words = clean_text(word_tokens) #Words list without punctuations and stop words\n",
    "        sent_tokens = sent_tokenize(textfile)   #List of sentences in the text\n",
    "        \n",
    "        \n",
    "        Num_clean_words = len(cleaned_words)    #Number of cleaned words\n",
    "        Num_sent = len(sent_tokens)             #Number of sentences\n",
    "\n",
    "        Positive_Score = positive_score(cleaned_words)                                #Computing Positive Score\n",
    "        Negative_Score = negative_score(cleaned_words)                                #Computing Negative Score\n",
    "        Polarity_Score = polarity_score(Positive_Score ,Negative_Score)               #Computing Polarity Score\n",
    "        Subjectivity_Score = subjectivity_score(Positive_Score ,Negative_Score ,cleaned_words)#Computing Subjectivity Score\n",
    "        cmplx_count = complex_words_count(cleaned_words)                              #Computing No. of Complex Words\n",
    "        Avg_sent_len = avg_sent_len(Num_clean_words,Num_sent)                                    #Average Sentence Length\n",
    "        Per_cmplx = per_cmplx(cmplx_count,Num_clean_words)                                      #Computing Percentage of Complex Words\n",
    "        Fog_index = fog_index(Per_cmplx,Avg_sent_len)                                          #Computing Fog Index\n",
    "        Pers_pronoun = pers_pronoun(word_tokens)                                              #Computing No. of Personal Pronouns\n",
    "        Avg_word_len = avg_word_len(cleaned_words,Num_clean_words)                           #Computing average word length\n",
    "        Syllable_per_word = total_syllable_count(cleaned_words)                             #Computing syllables per word\n",
    "        \n",
    "        #Append output to the Output file\n",
    "        Data ={'URL_ID':ID, 'URL':URL,'POSITIVE SCORE':Positive_Score, 'NEGATIVE SCORE': Negative_Score, 'POLARITY SCORE':Polarity_Score ,\n",
    "       'SUBJECTIVITY SCORE':Subjectivity_Score, 'AVG SENTENCE LENGTH':Avg_sent_len,\n",
    "       'PERCENTAGE OF COMPLEX WORDS':Per_cmplx, 'FOG INDEX':Fog_index,\n",
    "       'AVG NUMBER OF WORDS PER SENTENCE':Avg_sent_len, 'COMPLEX WORD COUNT':cmplx_count, 'WORD COUNT':Num_clean_words,\n",
    "       'SYLLABLE PER WORD':Syllable_per_word, 'PERSONAL PRONOUNS':Pers_pronoun, 'AVG WORD LENGTH':Avg_word_len}\n",
    "        Output_data.append(Data)  #appending rows in the form of dictionaries to a list\n",
    "    df = pd.DataFrame.from_records(Output_data) #creating a dataframe from list of rows as dictionaries\n",
    "    df.to_excel('output.xlsx' , index=False)  #creating excel file from DataFrame\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868db2f4-9598-4df6-aee7-1a540ec6f7cd",
   "metadata": {},
   "source": [
    "## RUN Programme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b737e-6b2d-4e48-abce-4381ab433c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaa5066a-e319-426c-90f5-e9bc78c0d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e81bb8-af73-4374-a972-b8ce9c331105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
